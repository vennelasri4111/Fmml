{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vennelasri4111/Fmml/blob/main/_FMML_M9L3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCpbL40ggQf1"
      },
      "source": [
        "# Module 9: Convolutional Neural Networks\n",
        "## **Lab 3**\n",
        "### Module coordinator: Aswin Jose and Anirvinya Gururajan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hAW8ptqVeyP"
      },
      "source": [
        "## 1. Understanding Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6wfvhccKxWx"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/464/0*e-SMFTzO8r7skkpc\" width=650px/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yZD5S7IQgHbU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BDE4WBHalreb"
      },
      "outputs": [],
      "source": [
        "# Importing some pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpRXyTpVv7u"
      },
      "source": [
        "Central to CNNs, a convolution operation is a linear element-wise multiplication operation between a small filter/kernel and same-sized patch from the image. We move this filter over the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image\n",
        "$f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amI6DTS0Ksvo"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "IalqupPPkDil",
        "outputId": "49ed7546-40c0-436c-94de-6859debc6604"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0], [1,1,0], [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0], [0,0,0], [1,1,1]])\n",
        "\n",
        "# On plotting the images\n",
        "plt.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()\n",
        "plt.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42INjCaketK",
        "outputId": "0c556de9-24a8-45c0-e888-26c7503b970b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  3\n",
            "Output from second image:  0\n"
          ]
        }
      ],
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tba3ySYUk2df",
        "outputId": "b2ee8f4d-7416-4dfc-9c18-11b2cf9d24e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  0\n",
            "Output from second image:  3\n"
          ]
        }
      ],
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmYcPhDgk_in"
      },
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image. Similarly, horizontal edge is detected in second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNdrDtAKqyj2"
      },
      "source": [
        "Let's define a function to use convolution layer from Pytorch and use our own kernel to detect edges in image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G5fRJziBk3YB"
      },
      "outputs": [],
      "source": [
        "def apply_conv(image, kernel, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER from Pytorch--------\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input.float())\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HPV6fFZloyc",
        "outputId": "a043bb6d-d1a5-4f10-c9b2-0d76ba55827d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not load image. Please check the file path.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Our original lotus image\n",
        "image = cv2.imread('grid1.jpg', 0)\n",
        "\n",
        "# Check if image loading was successful\n",
        "if image is None:\n",
        "    print(\"Error: Could not load image. Please check the file path.\")\n",
        "else:\n",
        "    filter = np.array([[-1, -1, -1],\n",
        "                       [0, 0, 0],\n",
        "                       [1, 1, 1]])\n",
        "\n",
        "    out1 = apply_conv(image, filter, padding=0, stride=1)\n",
        "\n",
        "    filter = np.array([[1, 0, -1],\n",
        "                       [1, 0, -1],\n",
        "                       [1, 0, -1]])\n",
        "\n",
        "    out2 = apply_conv(image, filter, padding=0, stride=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpA0yEk1BgRb"
      },
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n",
        "\n",
        "1) Max Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png' height=150px/>\n",
        "\n",
        "2) Average Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png' height=150px/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu3QIU7AEO_x"
      },
      "source": [
        "## Softmax layer/activation\n",
        "Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
        "\n",
        "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
        "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg' height=170px />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6grxC0TKKSF"
      },
      "source": [
        "## Learning to train a CNN network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qlO-uZUHnn_-"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnezCUbwGqzd",
        "outputId": "dade1685-655c-464a-c8ce-ff18e96e3344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "#  Images returned from torchvision dataset classes is in range [0,1]\n",
        "# We transform them to tensors and normalize them to range [-1,1] using 'Normalize' transform\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2M57DhHGupn",
        "outputId": "ab96a013-0a56-4e00-9b5d-de8d52162823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape :  (50000, 32, 32, 3) 50000\n",
            "Testing data shape :  (10000, 32, 32, 3) 10000\n",
            "Total number of outputs :  10\n",
            "Output classes :  ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
          ]
        }
      ],
      "source": [
        "print('Training data shape : ', trainset.data.shape, len(trainset.targets))\n",
        "print('Testing data shape : ', testset.data.shape, len(testset.targets))\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_haw697lHCZs"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x1Wi6vW7IHcR"
      },
      "outputs": [],
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RgxbRadcHIms"
      },
      "outputs": [],
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02meBxVOHLNL",
        "outputId": "266b170e-9b26-42be-a6e9-7f453af42738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Net()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lfKHypeYHNHO"
      },
      "outputs": [],
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MuDnJL28HPKP"
      },
      "outputs": [],
      "source": [
        "#(), lr:=0.05, momentum=0.9\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LCeRZ_qPmfU"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer) # Changed 'critertion' to 'criterion'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a_Q1Fs4voiio"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the CNN Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)          # Input: 3x32x32 -> Output: 16x30x30\n",
        "        self.pool = nn.MaxPool2d(2, 2)            # Output: 16x15x15\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)         # Output: 32x11x11\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)             # Output for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))      # After conv1 & pool\n",
        "        x = self.pool(F.relu(self.conv2(x)))      # After conv2 & pool\n",
        "        x = torch.flatten(x, 1)                   # Flatten the output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            loss = loss_func(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(output, 1)\n",
        "            running_correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = running_correct / total\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_acc.append(epoch_acc)\n",
        "\n",
        "        print('Epoch {}/{} | Loss: {:.4f} | Accuracy: {:.2f}%'.format(\n",
        "            epoch + 1, num_epochs, epoch_loss, epoch_acc * 100))\n",
        "\n",
        "    return train_losses, train_acc\n",
        "\n",
        "# Test function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the network: {:.2f}%'.format(100 * correct / total))\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# CIFAR-10 Dataset Loading\n",
        "# -----------------------------------------------\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                         download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Model, Loss, Optimizer\n",
        "# -----------------------------------------------\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Train the model\n",
        "# -----------------------------------------------\n",
        "num_epochs = 5\n",
        "train_losses, train_acc = train(num_epochs, model, train_loader, criterion, optimizer)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Test the model\n",
        "# -----------------------------------------------\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Plotting training loss and accuracy\n",
        "# -----------------------------------------------\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(np.arange(1, len(train_losses) + 1), train_losses, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.arange(1, len(train_acc) + 1), [acc * 100 for acc in train_acc], marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Accuracy (%)')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnxRZVrsojX3"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ... (Your existing code for Net class, train function, and test_model function) ...\n",
        "\n",
        "# Create an instance of the model\n",
        "model = Net()  # Define the model before using it\n",
        "\n",
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "# ... (Rest of your training and testing code) ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM2wHKGuHToB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ... previous code ...\n",
        "\n",
        "# Create an instance of the model\n",
        "model = Net()\n",
        "\n",
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5  # iterations\n",
        "\n",
        "# Train the model and get the training losses and accuracy\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer) # Call the train function\n",
        "\n",
        "# Plotting code\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(np.arange(1, len(train_losses) + 1), train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(np.arange(1, len(train_acc) + 1), train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sHK9hhmI-VY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Accuracy on test data after training\n",
        "test_model(model, testloader)\n",
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = Net()\n",
        "\n",
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5  # iterations\n",
        "\n",
        "# Training loop\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)\n",
        "\n",
        "\n",
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))\n",
        "\n",
        "#creating an instance of the model\n",
        "model = Net()\n",
        "\n",
        "#cross entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "#Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "#Training loop\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)\n",
        "\n",
        "# Accuracy on test data after training\n",
        "test_model(model, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBQeCEB6REnH"
      },
      "source": [
        "## Questions\n",
        "1) List some reasons why we should prefer CNN over ANN for image classification?\n",
        "\n",
        "2) Try improving the CNN performance further by tuning the hyperparameters(epochs, optimizer, LR etc). Report the improved test accuracy.\n",
        "\n",
        "3) What happens if you reduce the number of convolution layers to only 1?\n",
        "\n",
        "4) Why didn't we use the Softmax activation in the last layer of CNN?\n",
        "\n",
        "5) How does pooling help in this context ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fS6YwXs4yk_"
      },
      "source": [
        "1.List some reasons why we should prefer CNN over ANN for image classification?\n",
        "\n",
        "Answer:\n",
        "\n",
        "CNN vs ANN for Image Classification\n",
        "Here are some reasons why CNNs are preferred over ANNs for image classification:\n",
        "\n",
        "1. *Spatial Hierarchy*: CNNs can capture spatial hierarchies of features in images, which is essential for image classification.\n",
        "2. *Local Connectivity*: CNNs use local connectivity, which allows them to focus on specific regions of the image.\n",
        "3. *Shared Weights*: CNNs use shared weights, which reduces the number of parameters and improves generalization.\n",
        "4. *Translation Equivariance*: CNNs are translation equivariant, meaning that they can recognize features regardless of their position in the image.\n",
        "5. *Robustness to Variations*: CNNs can be robust to variations in image scale, rotation, and lighting condition\n",
        "\n",
        "Conclusion\n",
        "CNNs are well-suited for image classification tasks due to their ability to capture spatial hierarchies, local connectivity, and shared weights. These properties make CNNs more effective and efficient than ANNs for image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkNpS3jF5jnb"
      },
      "source": [
        "2.Try improving the CNN performance further by tuning the hyperparameters(epochs, optimizer, LR etc).\n",
        "\n",
        "\n",
        "Answer:\n",
        "Hyperparameter Tuning\n",
        "To improve the CNN performance, we can try tuning the hyperparameters:\n",
        "\n",
        "Hyperparameters to Tune\n",
        "1. *Epochs*: Increase or decrease the number of epochs to find the optimal value.\n",
        "2. *Optimizer*: Try different optimizers such as Adam, RMSprop, or SGD.\n",
        "3. *Learning Rate (LR)*: Adjust the learning rate to find the optimal value.\n",
        "4. *Batch Size*: Experiment with different batch sizes.\n",
        "\n",
        "Conclusion\n",
        "Hyperparameter tuning can significantly improve the performance of the CNN model. By experimenting with different hyperparameters, you can find the optimal combination that results in the best accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfwhKKfR7Zgk"
      },
      "source": [
        "3.What happens if you reduce the number of convolution layers to only 1?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Reducing Convolution Layers\n",
        "If you reduce the number of convolution layers to only 1:\n",
        "\n",
        "Effects\n",
        "1. *Reduced Feature Extraction*: With only one convolution layer, the model may not be able to extract complex features from the input data.\n",
        "2. *Decreased Representation Power*: A single convolution layer may not be sufficient to capture the underlying patterns and relationships in the data.\n",
        "3. *Potential Underfitting*: The model may underfit the training data, resulting in poor performance on both the training and testing sets.\n",
        "\n",
        "Possible Outcomes\n",
        "1. *Lower Accuracy*: The model's accuracy may decrease due to the reduced ability to extract features and represent complex relationships.\n",
        "2. *Increased Bias*: The model may exhibit increased bias towards certain features or patterns, leading to poor generalization.\n",
        "\n",
        "When to Use a Single Convolution Layer\n",
        "1. *Simple Problems*: A single convolution layer may be sufficient for simple image classification problems with limited complexity.\n",
        "2. *Limited Data*: When working with limited data, a single convolution layer may be a good starting point to avoid overfitting.\n",
        "\n",
        "Conclusion\n",
        "Reducing the number of convolution layers to only 1 can negatively impact the model's performance, especially for complex image classification tasks. However, it may be suitable for simple problems or when working with limited data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-YfdLjy75F9"
      },
      "source": [
        "4.Why didn't we use the Softmax activation in the last layer of CNN?\n",
        "\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Softmax Activation\n",
        "In the CNN model, we used the Softmax activation function implicitly through the `sparse_categorical_crossentropy` loss function.\n",
        "\n",
        "Why Not Explicit Softmax\n",
        "When using `sparse_categorical_crossentropy` or `categorical_crossentropy` loss functions in Keras, you don't need to explicitly define a Softmax activation function in the last layer.\n",
        "\n",
        "How it Works\n",
        "The Softmax activation function is applied internally by the loss function, which expects logits (unnormalized scores) as input.\n",
        "\n",
        "Explicit Softmax\n",
        "If you want to explicitly define a Softmax activation function, you can use it in the last layer:\n",
        "\n",
        "```\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "```\n",
        "\n",
        "In this case, you would use `categorical_crossentropy` loss function.\n",
        "\n",
        "Conclusion\n",
        "The choice between explicit and implicit Softmax activation depends on the loss function used. Both approaches can work, but it's essential to understand how the loss function interacts with the activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha3zSxqm9D2Q"
      },
      "source": [
        "5.Robustness to Variations: CNNs can be robust to variations in image scale, rotation, and lighting condition\n",
        "\n",
        "Answer:\n",
        "\n",
        "Robustness to Variations\n",
        "CNNs can be robust to variations in:\n",
        "\n",
        "1. *Image Scale*: CNNs can learn to recognize objects at different scales due to the use of convolutional and pooling layers.\n",
        "2. *Rotation*: While traditional CNNs are not inherently rotation-invariant, techniques like data augmentation and rotation-invariant layers can improve robustness to rotation.\n",
        "3. *Lighting Conditions*: CNNs can learn to recognize objects under different lighting conditions due to the use of convolutional layers that extract features from local regions.\n",
        "\n",
        "Achieving Robustness\n",
        "To achieve robustness to variations, CNNs can utilize:\n",
        "\n",
        "1. *Data Augmentation*: Applying random transformations (e.g., rotation, scaling, flipping) to training images.\n",
        "2. *Regularization Techniques*: Using techniques like dropout and weight decay to prevent overfitting.\n",
        "3. *Robust Loss Functions*: Using loss functions that are robust to outliers and variations.\n",
        "\n",
        "Limitations\n",
        "While CNNs can be robust to some variations, they may still struggle with:\n",
        "\n",
        "1. *Severe Occlusions*: Objects that are heavily occluded may be difficult for CNNs to recognize.\n",
        "2. *Large Viewpoint Changes*: Objects viewed from drastically different angles may require specialized architectures or training.\n",
        "\n",
        "Conclusion\n",
        "CNNs can be robust to various image variations, but achieving robustness often requires careful design, training, and regularization techniques."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}